{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mThXALJl9Yue",
    "tags": []
   },
   "source": [
    "# Tabular Workflows: TabNet Pipeline\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/brianchunkang/ai-samples/blob/master/tabnet/tabnet_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/brianchunkang/ai-samples/blob/master/tabnet/tabnet_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/brianchunkang/ai-samples/master/tabnet/tabnet_on_vertex_pipelines.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc745968395"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook showcases how to run the TabNet algorithm using Vertex AI Tabular Workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f887ec5c06c5"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to create two classification models using Vertex AI TabNet Tabular Workflows. Each workflow is a managed instance of [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Vertex AI Training\n",
    "- Vertex Pipelines\n",
    "- Cloud Storage\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a TabNet CustomJob. This is the best option if you know which hyperparameters to use for training.\n",
    "- Create a TabNet HyperparameterTuningJob. This allows you to get the best set of hyperparameters for your dataset.\n",
    "\n",
    "After training, each pipeline returns a link to the Vertex Model UI. You can use the UI to deploy the model, get online predictions, or run batch prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac26958afe8"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset you will be using is [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing).\n",
    "The data is for direct marketing campaigns (phone calls) of a Portuguese banking institution. The binary classification goal is to predict if a client will subscribe a term deposit. For this notebook, we randomly selected 90% of the rows in the original dataset and saved them in a train.csv file hosted on Cloud Storage. To download the file, click [here](https://storage.googleapis.com/cloud-samples-data/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuOonx6suOb7"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install --upgrade google-cloud-pipeline-components -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj5O0S5RTxzY"
   },
   "source": [
    "### Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "023DMKUaTypt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### GPU runtime\n",
    "\n",
    "This tutorial does not require a GPU runtime.\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the following APIs: Vertex AI APIs, Dataflow APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zebLBGXOky2A"
   },
   "source": [
    "## Notes about service account and permission\n",
    "\n",
    "**By default no configuration is required**, if you run into any permission related issue, please make sure the service accounts above have the required roles:\n",
    "\n",
    "|Service account email|Description|Roles|\n",
    "|---|---|---|\n",
    "|PROJECT_NUMBER-compute@developer.gserviceaccount.com|Compute Engine default service account|Dataflow Admin, Dataflow Worker, Storage Admin, BigQuery Admin, Vertex AI User|\n",
    "|service-PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com|AI Platform Service Agent|Vertex AI Service Agent|\n",
    "\n",
    "\n",
    "1. Goto https://console.cloud.google.com/iam-admin/iam.\n",
    "2. Check the \"Include Google-provided role grants\" checkbox.\n",
    "3. Find the above emails.\n",
    "4. Grant the corresponding roles.\n",
    "\n",
    "### Using data source from a different project\n",
    "- For the BQ data source, grant both service accounts the \"BigQuery Data Viewer\" role.\n",
    "- For the CSV data source, grant both service accounts the \"Storage Object Viewer\" role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95cb7ffd6895"
   },
   "source": [
    "### Set your project ID\n",
    "\n",
    "Set your project ID below. If you know know your project ID, leave the field blank and the following cells may be able to find it. Optionally, you may also set a service account in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd85f5c794e5"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a604eeffc32"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30e64c0eda41"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b12f508d97c6"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e8b7997de7a"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu0e2TRVxjHb"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. \n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "- In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
    "\n",
    "- **Click Create service account**.\n",
    "\n",
    "- In the **Service account name** field, enter a name, and click **Create**.\n",
    "\n",
    "- In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "- Click Create. A JSON file that contains your key downloads to your local environment.\n",
    "\n",
    "- Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nJ1VqdTxosw"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUfwWir9yPNV"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "All training related files (TF model checkpoint, TensorBoard file, etc) will be saved to the GCS bucket. The pipeline will not clean up the files since some of them might be useful for you, **please make sure to clean up the files**. For easy cleanup, you can set [GCS bucket level TTL](https://cloud.google.com/storage/docs/lifecycle).\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5325f437b46a"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "GENERATE_BUCKET_URI = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a30ea8551126"
   },
   "source": [
    "Create the bucket if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0217c63ed87f"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "if GENERATE_BUCKET_URI:\n",
    "    bucket_name = \"gs://test-{}\".format(uuid.uuid4())\n",
    "    !gsutil mb -p {PROJECT_ID} -l {REGION} {bucket_name}\n",
    "\n",
    "    # set GCS bucket object TTL to 7 days\n",
    "    !echo '{\"rule\":[{\"action\": {\"type\": \"Delete\"},\"condition\": {\"age\": 7}}]}' > gcs_lifecycle.tmp\n",
    "    !gsutil lifecycle set gcs_lifecycle.tmp {bucket_name}\n",
    "    !rm gcs_lifecycle.tmp\n",
    "\n",
    "    BUCKET_URI = bucket_name\n",
    "    print(f\"changed BUCKET_URI to {BUCKET_URI} due to GENERATE_BUCKET_URI is True\")\n",
    "\n",
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + uuid.uuid4()\n",
    "\n",
    "! gsutil ls -b $BUCKET_URI || gsutil mb -l $DATA_REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4cf2cdebb50"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96ad3d416327"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbbc3479a1da"
   },
   "source": [
    "## Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G6YmJT1yqkV"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google_cloud_pipeline_components.experimental.automl.tabular import \\\n",
    "    utils as automl_tabular_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0423f260423"
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad69f2590268"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LWH3PRF5o2v"
   },
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FPFT8c5oC0"
   },
   "outputs": [],
   "source": [
    "def get_model_artifacts_path(task_details: List[Dict[str, Any]], task_name: str) -> str:\n",
    "    task = get_task_detail(task_details, task_name)\n",
    "    return task.outputs[\"unmanaged_container_model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "def get_model_uri(task_details: List[Dict[str, Any]]) -> str:\n",
    "    task = get_task_detail(task_details, \"model-upload\")\n",
    "    # in format https://<location>-aiplatform.googleapis.com/v1/projects/<project_number>/locations/<location>/models/<model_id>\n",
    "    model_id = task.outputs[\"model\"].artifacts[0].uri.split(\"/\")[-1]\n",
    "    return f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model_id}?project={PROJECT_ID}\"\n",
    "\n",
    "\n",
    "def get_bucket_name_and_path(uri: str) -> str:\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "\n",
    "def download_from_gcs(uri: str) -> str:\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "\n",
    "def get_model_name(job_id: str) -> str:\n",
    "    pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "        job_id\n",
    "    ).gca_resource.job_detail.task_details\n",
    "    upload_task_details = get_task_detail(pipeline_task_details, \"model-upload\")\n",
    "    return upload_task_details.outputs[\"model\"].artifacts[0].metadata[\"resourceName\"]\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(\n",
    "    task_details: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvNFMRmBegZq"
   },
   "source": [
    "## Define training specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a7332a3f8e2"
   },
   "source": [
    "### Configure dataset\n",
    "\n",
    "You define either of the following parameters:\n",
    "\n",
    "- `data_source_csv_filenames`: The CSV data source.\n",
    "- `data_source_bigquery_table_path`: The BigQuery data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6dd1af1d336"
   },
   "outputs": [],
   "source": [
    "data_source_csv_filenames = \"gs://cloud-samples-data/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv\"\n",
    "data_source_bigquery_table_path = (None)  # @param {type:\"string\"}, format: bq://bq_project.bq_dataset.bq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkXNDwME9qv_"
   },
   "source": [
    "### Configure feature transformation\n",
    "\n",
    "Transformations can be specified using Feature Transform Engine (FTE) specific configurations. In the following, we provide some sample transform configurations to demonstrate FTE's capabilities:\n",
    "\n",
    "For a complete list of supported feature transformation configs and examples, please go [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.15/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.FeatureTransformEngineOp).\n",
    "\n",
    "- Full auto transformations (i.e., `auto_transform_config`): FTE automatically configure a set of built-in transformations for each input column based on its data statistics. \n",
    "- Fully specified transformations (i.e., `no_auto_transform_config`): All transformations on input columns are explicitly specified with FTE's built-in transformations. Chaining of multiple transformations on a single column is also supported.\n",
    "- Mix of auto and explicit transformations (i.e., `mixed_transform_config`).\n",
    "- Custom transformations (i.e., `transform_config_with_custom_transform`): A mixture of auto and explicit transformations and custom, bring-your-own transform function, where users can define and import their own transform function and use it with FTE's built-in transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fce334e09df6"
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"age\",\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"default\",\n",
    "    \"balance\",\n",
    "    \"housing\",\n",
    "    \"loan\",\n",
    "    \"contact\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"duration\",\n",
    "    \"campaign\",\n",
    "    \"pdays\",\n",
    "    \"previous\",\n",
    "    \"poutcome\",\n",
    "]\n",
    "\n",
    "auto_transform_config = {\"auto_transforms\": features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual, mixture, and custom transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Manual configuration\n",
    "## Code here: third_party/py/cloud_ml_autoflow/feature_transform_engine/builtin_transformations.py\n",
    "## ZScaleTransform - outputs a z-scale normalized input column.\n",
    "## Cast to Float - feature to Float\n",
    "## VocabularyTransform tokenizes string features \n",
    "no_auto_transform_config = {\n",
    "    'transforms': [{\n",
    "        'transform': 'ZScaleTransform',\n",
    "        'input_column_names': ['age']\n",
    "    }, {\n",
    "        'transform': 'CastToFloatTransform',\n",
    "        'input_column_names': ['balance'],\n",
    "        'output_column_names': ['balance']\n",
    "    }, {\n",
    "        'transform': 'ZScaleTransform',\n",
    "        'input_column_names': ['pdays']\n",
    "    }, {\n",
    "        'transform': 'VocabularyTransform',\n",
    "        'input_column_names': ['deposit'],\n",
    "        'output_column_names': ['deposit']\n",
    "    }]\n",
    "}\n",
    "\n",
    "mixed_transform_config = {\n",
    "    'auto_transforms': ['marital', 'education', 'default'],\n",
    "    'transforms': [{\n",
    "        'transform': 'ZScaleTransform',\n",
    "        'input_column_names': ['age']\n",
    "    }, {\n",
    "        'transform': 'CastToFloatTransform',\n",
    "        'input_column_names': ['balance'],\n",
    "        'output_column_names': ['balance']\n",
    "    }, {\n",
    "        'transform': 'ZScaleTransform',\n",
    "        'input_column_names': ['pdays']\n",
    "    }, {\n",
    "        'transform': 'VocabularyTransform',\n",
    "        'input_column_names': ['deposit'],\n",
    "        'output_column_names': ['deposit']\n",
    "    }]\n",
    "}\n",
    "\n",
    "#### Custom transform. Example python function here:\n",
    "'''\n",
    "def plus_one_transform(x: tf.SparseTensor) -> tf.SparseTensor:\n",
    "  return tf.SparseTensor(x.indices, tf.add(x.values, 1), x.dense_shape)\n",
    "'''\n",
    "transform_config_with_custom_transform = {\n",
    "    'auto_transforms': ['marital', 'education', 'default'],\n",
    "    'modules': [{\n",
    "        'transform': 'PlusOneTransform',\n",
    "        'module_path': 'gs://pvnguyen-us-central1/mp_notebook/custom_transform_fn.py',\n",
    "        'function_name': 'plus_one_transform'\n",
    "    }],\n",
    "    'transforms': [{\n",
    "        'transform': 'ZScaleTransform',\n",
    "        'input_column_names': ['age']\n",
    "    }, {\n",
    "        'transform': 'CastToFloatTransform',\n",
    "        'input_column_names': ['balance'],\n",
    "        'output_column_names': ['balance']\n",
    "    }, {\n",
    "        'transform': 'ZScaleTransform',\n",
    "        'input_column_names': ['pdays']\n",
    "    }, {\n",
    "        'transform': 'VocabularyTransform',\n",
    "        'input_column_names': ['deposit'],\n",
    "        'output_column_names': ['deposit']\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP3AQ3ERAH5n"
   },
   "source": [
    "Additional transformations to try out and their sample configurations:\n",
    "\n",
    "* `DatetimeTransform`:\n",
    "``` python\n",
    "# Outputs columns with granular datetime information (year, month, day, etc.).\n",
    "{\n",
    "    'transform': 'DatetimeTransform',\n",
    "    'input_column_names': ['feature_1'],\n",
    "    'time_format': '%Y-%m-%d'  # time format of input column\n",
    "}\n",
    "```\n",
    "\n",
    "* `LogTransform`:\n",
    "``` python\n",
    "# Outputs a column of the element-wise, natural logarithm of our input.\n",
    "{\n",
    "    'transform': 'LogTransform',\n",
    "    'input_column_names': ['feature_1']\n",
    "}\n",
    "```\n",
    "\n",
    "* `NGramTransform`:\n",
    "``` python\n",
    "# Outputs a column containing the vocab lookup incidies of n-grams in our\n",
    "# input.\n",
    "{\n",
    "    'transform': 'NGramTransform',\n",
    "    'input_column_names': ['feature_1'],\n",
    "    'min_ngram_size': 1,  # min number of tokens in our n-gram\n",
    "    'max_ngram_size': 2,  # max number of tokens in our n-gram\n",
    "    'separator': ' '  # seperator between tokens\n",
    "  }\n",
    "```\n",
    "* `ClipTransform`:\n",
    "``` python\n",
    "# Outputs a column where all values < min_value are assigned min_value\n",
    "# and all columns > max_value are assigned max_value.\n",
    "{\n",
    "    'transform': 'ClipTransform',\n",
    "    'input_column_names': ['col1'],\n",
    "    'output_column_names': ['col1_clipped'],\n",
    "    'min_value': 1.,\n",
    "    'max_value': 10.,\n",
    "}\n",
    "```\n",
    "* `MaxAbsScaleTransform`:\n",
    "``` python\n",
    "# Outputs a column where all input elements are divided by abs(max(input)).\n",
    "{\n",
    "    'transform': 'MaxAbsScaleTransform',\n",
    "    'input_column_names': ['col1'],\n",
    "    'output_column_names': ['col1_max_abs_scaled']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b28b609b259"
   },
   "source": [
    "### Setup training configuration\n",
    "\n",
    "You define the following:\n",
    "\n",
    "- `target_column`: The target column name.\n",
    "- `prediction_type`: The type of prediction the model is to produce.\n",
    "  'classification' or 'regression'.\n",
    "- `transform_config`: The path to a GCS file containing the transformations to apply.\n",
    "- `weight_column`: The weight column name. Leave at None here, as TabNet's attention mechanism addresses this\n",
    "- `run_evaluation`: Whether to run evaluation steps during training.\n",
    "\n",
    "Split percentages for train/val/test\n",
    "- `training_fraction`: The training fraction.\n",
    "- `validation_fraction`: The validation fraction.\n",
    "- `test_fraction`: The test fraction.\n",
    "\n",
    "When timestamp_split_key is specified, the train/val/test split will generate according to the split percentages with the train split happening before validation, which happens before the test split\n",
    "- `timestamp_split_key`: The timestamp_split column name.\n",
    "\n",
    "When the stratified_split_key is specified, stratified split will be generated according to the stratified_split_key and the defined fraction.\n",
    "- `stratified_split_key`: The stratified_split column name. Provide a manually generated column related to the target column, in order to split data so that each target class has the same percentage of records\n",
    "\n",
    "****OR****\n",
    "\n",
    "Use the predefined split key:\n",
    "- `predefined_split_key`: The name of the column to use to split the dataset. Acceptable values in this column include `training`, `validation`, and `test`.\n",
    "\n",
    "****For a full list of Training Job parameters, see [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters).****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eV4JrwB8wAkg"
   },
   "outputs": [],
   "source": [
    "run_evaluation = True  # @param {type:\"boolean\"}\n",
    "prediction_type = \"classification\"\n",
    "target_column = \"deposit\"\n",
    "\n",
    "# Fraction split\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "timestamp_split_key = None  # timestamp column name when using timestamp split\n",
    "stratified_split_key = None  # target column name when using stratified split\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None\n",
    "\n",
    "transform_config = auto_transform_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyWGg2s09xOk"
   },
   "source": [
    "## VPC related config\n",
    "\n",
    "You define the following:\n",
    "\n",
    "- `dataflow_subnetwork`: Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used. Example:\n",
    "https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications\n",
    "- `dataflow_use_public_ips`: Specifies whether Dataflow workers use public IP\n",
    "  addresses.\n",
    "\n",
    "If you need to use a custom Dataflow subnetwork, you can set it through the `dataflow_subnetwork` parameter. The requirements are:\n",
    "1. `dataflow_subnetwork` must be fully qualified subnetwork name.\n",
    "   [[reference](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)]\n",
    "1. The following service accounts must have [Compute Network User role](https://cloud.google.com/compute/docs/access/iam#compute.networkUser) assigned on the specified dataflow subnetwork [[reference](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared)]:\n",
    "    1. Compute Engine default service account: PROJECT_NUMBER-compute@developer.gserviceaccount.com\n",
    "    1. Dataflow service account: service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
    "\n",
    "If your project has VPC-SC enabled, please make sure:\n",
    "\n",
    "1. The dataflow subnetwork used in VPC-SC is configured properly for Dataflow.\n",
    "   [[reference](https://cloud.google.com/dataflow/docs/guides/routes-firewall)]\n",
    "1. `dataflow_use_public_ips` is set to False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TePNlLl9v1q"
   },
   "outputs": [],
   "source": [
    "dataflow_subnetwork = \"\"  # @param {type:\"string\"}\n",
    "dataflow_use_public_ips = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sF8a2RKtRhg"
   },
   "source": [
    "## Customize TabNet HyperparameterTuningJob configuration and create pipeline\n",
    "\n",
    "To get the best set of hyperparameters for your dataset, we recommend running a HyperparameterTuningJob.\n",
    "\n",
    "Hyperparameters that can be tuned are set in the optional `study_spec_parameters_override` parameter. We provide a helper function called `get_tabnet_study_spec_parameters_override` to get these hyperparameters. You provide `dataset_size_bucket` (one of 'small' (< 1M rows), 'medium' (1M - 100M rows), or 'large' (> 100M rows)), `training_budget_bucket` (one of 'small' (< \\\\$600), 'medium' (\\\\$600 - \\\\$2400), or 'large' (> \\\\$2400)), and `prediction_type` and Vertex AI returns a list of hyperparameters and ranges. `study_spec_parameters_override` can be empty or one or more of these hyperparameters can be specified. For hyperparameters not specified in `study_spec_parameters_override`, we set ranges in the pipeline. For a full list of hyperparameters available for tuning, see [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_spec_parameters_override = (\n",
    "    automl_tabular_utils.get_tabnet_study_spec_parameters_override(\n",
    "        dataset_size_bucket=\"small\",\n",
    "        prediction_type=prediction_type,\n",
    "        training_budget_bucket=\"small\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (study_spec_parameters_override)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbf2cf98842b"
   },
   "source": [
    "#### Take a look at the auto generated spec in YAML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "with open('study_spec.yaml', 'w') as f:\n",
    "    yaml.dump(study_spec_parameters_override, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat study_spec.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define HyperParameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters that have the largest impact on model size\n",
    "- `feature_dim`: Dimensionality of the hidden representation in feature transformation block. For large datasets (100s of GB, TB), higher values can help improve performance. 10% increase for some customer use cases where maximum value was > 500 and ranged from [100,500+]\n",
    "- `feature_dim_ratio`: The ratio of output dimension (dimensionality of the outputs of each decision step) to feature dimension. feature_dim_ratio can simply be around 0.5, e.g. in [0.2, 0.8]. \n",
    "- `num_decision_steps`: Number of sequential decision steps. [3, 10] is optimal, with more more information-bearing features, the higher steps is more valuable\n",
    "- `num_transformer_layers`: The number of transformer layers for each decision step. used only at one decision step and as it increases, more flexibility is provided to use a feature at multiple decision steps.\n",
    "- `batch_size`: Batch size for training. Larger batch sizes (e.g. 60K+) can improve model quality, but may require more GPUs, memory, and use of caching to disk\n",
    "\n",
    "### Other useful hyperparameters to be aware of\n",
    "- `large_category_dim`: Embedding dimension for categorical feature with large number of categories. This is to set multi dimensional embedding, instead of 1-D embedding. The default is 1. We suggest increasing it (e.g. to ~5 in most cases and even ~10 if the number of categories is typically very large in the dataset), if pushing the accuracy is the main goal, rather than computational efficiency and explainability.\n",
    "- `large category thresh`: Threshold for number of categories to apply large_category_dim embedding dimension to. If the number of distinct categories for a categorical column is greater than large_category_thresh we use a large_category_dim dimensional embedding, instead of 1-D embedding. The default is 300. We suggest decreasing it (e.g. to ~10), if pushing the accuracy is the main goal, rather than computational efficiency and explainability.\n",
    "- `relaxation_factor`: Relaxation factor that promotes the reuse of each feature at different decision steps. When it is 1, a feature is enforced to be used only at one decision step and as it increases, more flexibility is provided to use a feature at multiple decision steps. [1.2, 2.5] is often a good choice], can be lower for smaller/imbalanced datasets and higher for datasets with many features\n",
    "\n",
    "In addition to hyperparameters, HyperparameterTuningJob takes the following values in the example below:\n",
    "\n",
    "- `root_dir`: The root GCS directory for the pipeline components.\n",
    "- `worker_pool_specs_override`: The dictionary for overriding training and evaluation worker pool specs. The dictionary should be of [this format]( https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172). TabNet supports both CPU and GPU training.\n",
    "- `study_spec_metric_id`: Metric to optimize, possible values: ['loss', 'average_loss', 'rmse', 'mae', 'mql', 'accuracy', 'auc', 'precision', 'recall'].\n",
    "- `study_spec_metric_goal`: Optimization goal of the metric, possible values: \"MAXIMIZE\", \"MINIMIZE\".\n",
    "- `max_trial_count`: The desired total number of trials.\n",
    "- `parallel_trial_count`: The desired number of trials to run in parallel.\n",
    "- `max_failed_trial_count`: The number of failed trials that need to be seen before failing the HyperparameterTuningJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.\n",
    "\n",
    "For a full list of HyperparameterTuningJob parameters, see [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_hyperparameter_tuning_job_pipeline_and_parameters).\n",
    "\n",
    "Multiple trials can be configured. The pipeline returns the best trial based on the metric configured in `study_spec_metrics`. In the example below, we return the trial with the lowest loss value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize the study spec and run again\n",
    "\n",
    "Open the [HyperPredict Colab (Only available from Google Corp](https://colab.corp.google.com/drive/1Vf6ZglZXQqAKKeCZbhZk7UkkzFzAwRq_?resourcekey=0-fARLG4k0xtuh2QtI39-rPQ&usp=sharing) and get the study spec settings\n",
    "\n",
    "Adjust the study spec to the maximum values forthe values in order to test the machine configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile study_spec.json\n",
    "[\n",
    "  {\n",
    "    \"parameter_id\": \"max_steps\",\n",
    "    \"integer_value_spec\": {\n",
    "      \"min_value\": 51404,\n",
    "      \"max_value\": 51404\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"feature_dim\",\n",
    "    \"integer_value_spec\": {\n",
    "      \"min_value\": 257,\n",
    "      \"max_value\": 257\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"feature_dim_ratio\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.45,\n",
    "      \"max_value\": 0.45\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"num_decision_steps\",\n",
    "    \"integer_value_spec\": {\n",
    "      \"min_value\": 5,\n",
    "      \"max_value\": 5\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"num_transformer_layers\",\n",
    "    \"integer_value_spec\": {\n",
    "      \"min_value\": 5,\n",
    "      \"max_value\": 5\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"num_transformer_layers_ratio\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.677095895847249,\n",
    "      \"max_value\": 0.677095895847249\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"relaxation_factor\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 2.3080036197688507,\n",
    "      \"max_value\": 2.3080036197688507\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"decay_every\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 25702,\n",
    "      \"max_value\": 25702\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LOG_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"decay_rate\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.999,\n",
    "      \"max_value\": 0.999\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"batch_momentum\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.95,\n",
    "      \"max_value\": 0.95\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"learning_rate\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.001,\n",
    "      \"max_value\": 0.001\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LOG_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"batch_size_ratio\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.5568732550002481,\n",
    "      \"max_value\": 0.5568732550002481\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"sparsity_loss_weight\",\n",
    "    \"double_value_spec\": {\n",
    "      \"min_value\": 0.5,\n",
    "      \"max_value\": 0.5\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LOG_SCALE\"\n",
    "  },\n",
    "  {\n",
    "    \"parameter_id\": \"batch_size\",\n",
    "    \"integer_value_spec\": {\n",
    "      \"min_value\": 1356,\n",
    "      \"max_value\": 1356\n",
    "    },\n",
    "    \"scale_type\": \"UNIT_LINEAR_SCALE\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Opening JSON file\n",
    "f = open('study_spec.json')\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "study_spec_parameters_override = json.load(f)\n",
    "print (study_spec_parameters_override)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 1 trial with maximum parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_root_dir = os.path.join(BUCKET_URI, \"tabnet_hyperparameter_max_parameters_job\")\n",
    "\n",
    "transform_config_path = os.path.join(pipeline_job_root_dir, \"transform_config.json\")\n",
    "write_to_gcs(transform_config_path, json.dumps(transform_config))\n",
    "\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "\n",
    "# To test GPU training, the worker_pool_specs_override can be specified like this.\n",
    "# worker_pool_specs_override =  [\n",
    "#    {\n",
    "#       \"machine_spec\":{\n",
    "#          \"machine_type\":\"n1-highmem-32\",\n",
    "#          \"accelerator_type\":\"NVIDIA_TESLA_V100\",\n",
    "#          \"accelerator_count\":2\n",
    "#       }\n",
    "#    }\n",
    "# ]\n",
    "\n",
    "study_spec_metric_id = \"loss\"\n",
    "study_spec_metric_goal = \"MINIMIZE\"\n",
    "\n",
    "max_trial_count=1\n",
    "parallel_trial_count=1\n",
    "max_failed_trial_count=0\n",
    "\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_tabnet_hyperparameter_tuning_job_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    transform_config=transform_config_path,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    study_spec_metric_id=study_spec_metric_id,\n",
    "    study_spec_metric_goal=study_spec_metric_goal,\n",
    "    study_spec_parameters_override=study_spec_parameters_override,\n",
    "    max_trial_count=max_trial_count,\n",
    "    parallel_trial_count=parallel_trial_count,\n",
    "    max_failed_trial_count=max_failed_trial_count,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=True,\n",
    ")\n",
    "\n",
    "pipeline_job_id = f\"tabnet-hpt-max-parameters-{uuid.uuid4()}\"\n",
    "# More info on parameters PipelineJob accepts:\n",
    "# https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Run full run using 200 max trials and 10 parallel trials as a sample run\n",
    "#### Adjust the trial variables to the following and rerun. Remember to change the job name\n",
    "****max_trial_count=200, parallel_trial_count=10, max_failed_trial_count=10****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the best hpt parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the hyperparameter task details from the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabnet_hpt_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(\n",
    "        tabnet_hpt_pipeline_task_details, \"get-best-hyperparameter-tuning-job-trial\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_task_detail = get_task_detail(tabnet_hpt_pipeline_task_details, \"get-best-hyperparameter-tuning-job-trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hpt_task_detail.execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom functions to pull the hyperparameter job details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for md in hpt_task_detail.execution.metadata:\n",
    "    if (md==\"input:gcp_resources\"):\n",
    "        resource_str = hpt_task_detail.execution.metadata[md]\n",
    "        resource_dict = json.loads(resource_str)\n",
    "        print (resource_dict)\n",
    "        print (type(resource_dict))\n",
    "        hpt_pipeline_uri = resource_dict[\"resources\"][0][\"resourceUri\"]\n",
    "        print (hpt_pipeline_uri)\n",
    "        hpt_pipeline_jobId = hpt_pipeline_uri.rsplit('/', 1)[1]\n",
    "        print (hpt_pipeline_jobId)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_pipeline_job = aiplatform.HyperparameterTuningJob.get (hpt_pipeline_jobId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 1000 # Set to a low/high value depending on the metric. High when targeting low values, and vice versa\n",
    "best_trial = 0\n",
    "for trial in hpt_pipeline_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) < best_metric:\n",
    "        try:\n",
    "            best_metric = float(trial.final_measurement.metrics[0].value)\n",
    "            best_trial = int(trial.id) - 1\n",
    "        except:\n",
    "            best_metric = best_metric\n",
    "            best_trail = best_trial\n",
    "\n",
    "print(hpt_pipeline_job.trials[best_trial])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters to the values from the best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For reference, these are sample values from one of the runs above\n",
    "batch_momentum = 0.725\n",
    "batch_size = 2048.0\n",
    "batch_size_ratio = 0.25\n",
    "class_weight = 50.5\n",
    "decay_every = 3000\n",
    "decay_rate = 0.7067531393633848\n",
    "feature_dim = 125.0\n",
    "feature_dim_ratio = 0.5\n",
    "large_category_dim = 5.0\n",
    "large_category_thresh = 10.0\n",
    "learning_rate = 0.0014142135623730948\n",
    "loss_function_type = \"weighted_cross_entropy\"\n",
    "max_steps = 20000.0\n",
    "max_train_secs = -1.0\n",
    "num_decision_steps = 4.0\n",
    "num_transformer_layers = 3.0\n",
    "num_transformer_layers_ratio = 0.5\n",
    "relaxation_factor= 1.85\n",
    "sparsity_loss_weight = 0.00003162277\n",
    "\n",
    "#### Setting hyperparameters to the best values using the best trial values\n",
    "batch_momentum = hpt_pipeline_job.trials[best_trial].parameters[0].value\n",
    "batch_size = hpt_pipeline_job.trials[best_trial].parameters[1].value\n",
    "batch_size_ratio = hpt_pipeline_job.trials[best_trial].parameters[2].value\n",
    "class_weight = hpt_pipeline_job.trials[best_trial].parameters[3].value\n",
    "decay_every = hpt_pipeline_job.trials[best_trial].parameters[4].value\n",
    "decay_rate = hpt_pipeline_job.trials[best_trial].parameters[5].value\n",
    "feature_dim = hpt_pipeline_job.trials[best_trial].parameters[6].value\n",
    "feature_dim_ratio = hpt_pipeline_job.trials[best_trial].parameters[7].value\n",
    "large_category_dim = hpt_pipeline_job.trials[best_trial].parameters[8].value\n",
    "large_category_thresh = hpt_pipeline_job.trials[best_trial].parameters[9].value\n",
    "learning_rate = hpt_pipeline_job.trials[best_trial].parameters[10].value\n",
    "loss_function_type = f\"{hpt_pipeline_job.trials[best_trial].parameters[11].value}\"\n",
    "max_steps = hpt_pipeline_job.trials[best_trial].parameters[12].value\n",
    "max_train_secs = hpt_pipeline_job.trials[best_trial].parameters[13].value\n",
    "num_decision_steps = hpt_pipeline_job.trials[best_trial].parameters[14].value\n",
    "num_transformer_layers = hpt_pipeline_job.trials[best_trial].parameters[15].value\n",
    "num_transformer_layers_ratio = hpt_pipeline_job.trials[best_trial].parameters[16].value\n",
    "relaxation_factor= hpt_pipeline_job.trials[best_trial].parameters[17].value\n",
    "sparsity_loss_weight = hpt_pipeline_job.trials[best_trial].parameters[18].value\n",
    "sparsity_loss_weight = \"{:.20f}\".format(sparsity_loss_weight)\n",
    "\n",
    "print (batch_momentum)\n",
    "print (batch_size)\n",
    "print (batch_size_ratio)\n",
    "print (class_weight)\n",
    "print (decay_every)\n",
    "print (decay_rate)\n",
    "print (feature_dim)\n",
    "print (feature_dim_ratio)\n",
    "print (large_category_dim)\n",
    "print (large_category_thresh)\n",
    "print (learning_rate)\n",
    "print (loss_function_type)\n",
    "print (max_steps)\n",
    "print (max_train_secs)\n",
    "print (num_decision_steps)\n",
    "print (num_transformer_layers)\n",
    "print (num_transformer_layers_ratio)\n",
    "print (relaxation_factor)\n",
    "print (sparsity_loss_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use custom functions to pull the hyperparameter job details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for md in hpt_task_detail.execution.metadata:\n",
    "    if (md==\"input:gcp_resources\"):\n",
    "        resource_str = hpt_task_detail.execution.metadata[md]\n",
    "        resource_dict = json.loads(resource_str)\n",
    "        print (resource_dict)\n",
    "        print (type(resource_dict))\n",
    "        hpt_pipeline_uri = resource_dict[\"resources\"][0][\"resourceUri\"]\n",
    "        print (hpt_pipeline_uri)\n",
    "        hpt_pipeline_jobId = hpt_pipeline_uri.rsplit('/', 1)[1]\n",
    "        print (hpt_pipeline_jobId)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_pipeline_job = aiplatform.HyperparameterTuningJob.get (hpt_pipeline_jobId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 1000 # Set to a low/high value depending on the metric. High when targeting low values, and vice versa\n",
    "best_trial = 0\n",
    "for trial in hpt_pipeline_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) < best_metric:\n",
    "        try:\n",
    "            best_metric = float(trial.final_measurement.metrics[0].value)\n",
    "            best_trial = int(trial.id) - 1\n",
    "        except:\n",
    "            best_metric = best_metric\n",
    "            best_trail = best_trial\n",
    "\n",
    "print(hpt_pipeline_job.trials[best_trial])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters to the values from the best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For reference, these are sample values from one of the runs above\n",
    "batch_momentum = 0.725\n",
    "batch_size = 2048.0\n",
    "batch_size_ratio = 0.25\n",
    "class_weight = 50.5\n",
    "decay_every = 3000\n",
    "decay_rate = 0.7067531393633848\n",
    "feature_dim = 125.0\n",
    "feature_dim_ratio = 0.5\n",
    "large_category_dim = 5.0\n",
    "large_category_thresh = 10.0\n",
    "learning_rate = 0.0014142135623730948\n",
    "loss_function_type = \"weighted_cross_entropy\"\n",
    "max_steps = 20000.0\n",
    "max_train_secs = -1.0\n",
    "num_decision_steps = 4.0\n",
    "num_transformer_layers = 3.0\n",
    "num_transformer_layers_ratio = 0.5\n",
    "relaxation_factor= 1.85\n",
    "sparsity_loss_weight = 0.00003162277\n",
    "\n",
    "#### Setting hyperparameters to the best values using the best trial values\n",
    "batch_momentum = hpt_pipeline_job.trials[best_trial].parameters[0].value\n",
    "batch_size = hpt_pipeline_job.trials[best_trial].parameters[1].value\n",
    "batch_size_ratio = hpt_pipeline_job.trials[best_trial].parameters[2].value\n",
    "class_weight = hpt_pipeline_job.trials[best_trial].parameters[3].value\n",
    "decay_every = hpt_pipeline_job.trials[best_trial].parameters[4].value\n",
    "decay_rate = hpt_pipeline_job.trials[best_trial].parameters[5].value\n",
    "feature_dim = hpt_pipeline_job.trials[best_trial].parameters[6].value\n",
    "feature_dim_ratio = hpt_pipeline_job.trials[best_trial].parameters[7].value\n",
    "large_category_dim = hpt_pipeline_job.trials[best_trial].parameters[8].value\n",
    "large_category_thresh = hpt_pipeline_job.trials[best_trial].parameters[9].value\n",
    "learning_rate = hpt_pipeline_job.trials[best_trial].parameters[10].value\n",
    "loss_function_type = f\"{hpt_pipeline_job.trials[best_trial].parameters[11].value}\"\n",
    "max_steps = hpt_pipeline_job.trials[best_trial].parameters[12].value\n",
    "max_train_secs = hpt_pipeline_job.trials[best_trial].parameters[13].value\n",
    "num_decision_steps = hpt_pipeline_job.trials[best_trial].parameters[14].value\n",
    "num_transformer_layers = hpt_pipeline_job.trials[best_trial].parameters[15].value\n",
    "num_transformer_layers_ratio = hpt_pipeline_job.trials[best_trial].parameters[16].value\n",
    "relaxation_factor= hpt_pipeline_job.trials[best_trial].parameters[17].value\n",
    "sparsity_loss_weight = hpt_pipeline_job.trials[best_trial].parameters[18].value\n",
    "sparsity_loss_weight = \"{:.20f}\".format(sparsity_loss_weight)\n",
    "\n",
    "print (batch_momentum)\n",
    "print (batch_size)\n",
    "print (batch_size_ratio)\n",
    "print (class_weight)\n",
    "print (decay_every)\n",
    "print (decay_rate)\n",
    "print (feature_dim)\n",
    "print (feature_dim_ratio)\n",
    "print (large_category_dim)\n",
    "print (large_category_thresh)\n",
    "print (learning_rate)\n",
    "print (loss_function_type)\n",
    "print (max_steps)\n",
    "print (max_train_secs)\n",
    "print (num_decision_steps)\n",
    "print (num_transformer_layers)\n",
    "print (num_transformer_layers_ratio)\n",
    "print (relaxation_factor)\n",
    "print (sparsity_loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = 1000 # Set to a low/high value depending on the metric. High when targeting low values, and vice versa\n",
    "best_trial = 0\n",
    "for trial in pipeline_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) < best_metric:\n",
    "        try:\n",
    "            best_metric = float(trial.final_measurement.metrics[0].value)\n",
    "            best_trial = int(trial.id) - 1\n",
    "        except:\n",
    "            best_metric = best_metric\n",
    "            best_trail = best_trial\n",
    "\n",
    "print(pipeline_job.trials[best_trial])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters to the values from the best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For reference, these are sample values from one of the runs above\n",
    "batch_momentum = 0.725\n",
    "batch_size = 2048.0\n",
    "batch_size_ratio = 0.25\n",
    "class_weight = 50.5\n",
    "decay_every = 3000\n",
    "decay_rate = 0.7067531393633848\n",
    "feature_dim = 125.0\n",
    "feature_dim_ratio = 0.5\n",
    "large_category_dim = 5.0\n",
    "large_category_thresh = 10.0\n",
    "learning_rate = 0.0014142135623730948\n",
    "loss_function_type = \"weighted_cross_entropy\"\n",
    "max_steps = 20000.0\n",
    "max_train_secs = -1.0\n",
    "num_decision_steps = 4.0\n",
    "num_transformer_layers = 3.0\n",
    "num_transformer_layers_ratio = 0.5\n",
    "relaxation_factor= 1.85\n",
    "sparsity_loss_weight = 0.00003162277\n",
    "\n",
    "#### Setting hyperparameters to the best values using the best trial values\n",
    "batch_momentum = pipeline_job.trials[best_trial].parameters[0].value\n",
    "batch_size = pipeline_job.trials[best_trial].parameters[1].value\n",
    "batch_size_ratio = pipeline_job.trials[best_trial].parameters[2].value\n",
    "class_weight = pipeline_job.trials[best_trial].parameters[3].value\n",
    "decay_every = pipeline_job.trials[best_trial].parameters[4].value\n",
    "decay_rate = pipeline_job.trials[best_trial].parameters[5].value\n",
    "feature_dim = pipeline_job.trials[best_trial].parameters[6].value\n",
    "feature_dim_ratio = pipeline_job.trials[best_trial].parameters[7].value\n",
    "large_category_dim = pipeline_job.trials[best_trial].parameters[8].value\n",
    "large_category_thresh = pipeline_job.trials[best_trial].parameters[9].value\n",
    "learning_rate = pipeline_job.trials[best_trial].parameters[10].value\n",
    "loss_function_type = f\"{pipeline_job.trials[best_trial].parameters[11].value}\"\n",
    "max_steps = pipeline_job.trials[best_trial].parameters[12].value\n",
    "max_train_secs = pipeline_job.trials[best_trial].parameters[13].value\n",
    "num_decision_steps = pipeline_job.trials[best_trial].parameters[14].value\n",
    "num_transformer_layers = pipeline_job.trials[best_trial].parameters[15].value\n",
    "num_transformer_layers_ratio = pipeline_job.trials[best_trial].parameters[16].value\n",
    "relaxation_factor= pipeline_job.trials[best_trial].parameters[17].value\n",
    "sparsity_loss_weight = pipeline_job.trials[best_trial].parameters[18].value\n",
    "sparsity_loss_weight = \"{:.20f}\".format(sparsity_loss_weight)\n",
    "\n",
    "print (batch_momentum)\n",
    "print (batch_size)\n",
    "print (batch_size_ratio)\n",
    "print (class_weight)\n",
    "print (decay_every)\n",
    "print (decay_rate)\n",
    "print (feature_dim)\n",
    "print (feature_dim_ratio)\n",
    "print (large_category_dim)\n",
    "print (large_category_thresh)\n",
    "print (learning_rate)\n",
    "print (loss_function_type)\n",
    "print (max_steps)\n",
    "print (max_train_secs)\n",
    "print (num_decision_steps)\n",
    "print (num_transformer_layers)\n",
    "print (num_transformer_layers_ratio)\n",
    "print (relaxation_factor)\n",
    "print (sparsity_loss_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iXXE14voyR"
   },
   "source": [
    "## Customize TabNet CustomJob configuration and create pipeline\n",
    "\n",
    "This is best choice if you know exactly which hyperparameter values to use for model training. It uses fewer training resources than a HyperparameterTuningJob. \n",
    "\n",
    "In the example below, you configure the following:\n",
    "\n",
    "- `root_dir`: The root GCS directory for the pipeline components.\n",
    "- `worker_pool_specs_override`: The dictionary for overriding training and evaluation worker pool specs. The dictionary should be of [this format]( https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172). TabNet supports both CPU and GPU training.\n",
    "- `learning_rate`: The learning rate used by the linear optimizer.\n",
    "- `max_steps`: Number of steps to run the trainer for.\n",
    "- `max_train_secs`: Amount of time in seconds to run the trainer for.\n",
    "\n",
    "A complete list of pipeline inputs and model hyperparameters is available [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG46cXVueb66"
   },
   "outputs": [],
   "source": [
    "pipeline_job_root_dir = os.path.join(BUCKET_URI, \"tabnet_custom_job\")\n",
    "\n",
    "transform_config_path = os.path.join(pipeline_job_root_dir, \"transform_config.json\")\n",
    "write_to_gcs(transform_config_path, json.dumps(transform_config))\n",
    "\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-8\"}}  # Override for TF chief node\n",
    "]\n",
    "\n",
    "# To test GPU training, the worker_pool_specs_override can be specified like this.\n",
    "# worker_pool_specs_override =  [\n",
    "#     {\"machine_spec\": {\n",
    "#       'machine_type': \"n1-highmem-32\",\n",
    "#       \"accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "#       \"accelerator_count\": 2\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_tabnet_trainer_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    #### Add best trial parameters ####\n",
    "    batch_momentum=batch_momentum,\n",
    "    batch_size=batch_size,\n",
    "    batch_size_ratio=batch_size_ratio,\n",
    "    class_weight=class_weight,\n",
    "    decay_every=decay_every,\n",
    "    decay_rate=decay_rate,\n",
    "    feature_dim=feature_dim,\n",
    "    feature_dim_ratio=feature_dim_ratio,\n",
    "    large_category_dim=large_category_dim,\n",
    "    large_category_thresh=large_category_thresh,\n",
    "    learning_rate=learning_rate,\n",
    "    loss_function_type=loss_function_type,\n",
    "    max_steps=max_steps,\n",
    "    max_train_secs=max_train_secs,\n",
    "    num_decision_steps=num_decision_steps,\n",
    "    num_transformer_layers=num_transformer_layers,\n",
    "    num_transformer_layers_ratio=num_transformer_layers_ratio,\n",
    "    relaxation_factor=relaxation_factor,\n",
    "    sparsity_loss_weight=sparsity_loss_weight,\n",
    "    #### Set remaining parameters ####\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    transform_config=transform_config_path,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=run_evaluation,\n",
    ")\n",
    "\n",
    "pipeline_job_id = f\"tabnet-{uuid.uuid4()}\"\n",
    "# More info on parameters PipelineJob accepts:\n",
    "# https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbf2cf98842b"
   },
   "source": [
    "### Deploy the endpoint using the SDK\n",
    "You can also deploy the model and test online prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model(CUSTOM_JOB_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy training file to review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp $data_source_csv_filenames ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[{\"age\":\"38\",\"job\":\"blue-collar\",\"marital\":\"divorced\",\"education\":\"secondary\",\"default\":\"no\",\"balance\":\"2998\",\"housing\":\"yes\",\"loan\":\"no\",\"contact\":\"unknown\",\"day\":\"11\",\"month\":\"jun\",\"duration\":\"91\",\"campaign\":\"1\",\"pdays\":\"-1\",\"previous\":\"0\",\"poutcome\":\"unknown\"},\n",
    "                            {\"age\":\"30\",\"job\":\"services\",\"marital\":\"married\",\"education\":\"secondary\",\"default\":\"no\",\"balance\":\"586\",\"housing\":\"yes\",\"loan\":\"no\",\"contact\":\"cellular\",\"day\":\"20\",\"month\":\"nov\",\"duration\":\"148\",\"campaign\":\"6\",\"pdays\":\"176\",\"previous\":\"4\",\"poutcome\":\"failure\"},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile instances.json\n",
    "{\"instances\":[{\"age\":\"38\",\"job\":\"blue-collar\",\"marital\":\"divorced\",\"education\":\"secondary\",\"default\":\"no\",\"balance\":\"2998\",\"housing\":\"yes\",\"loan\":\"no\",\"contact\":\"unknown\",\"day\":\"11\",\"month\":\"jun\",\"duration\":\"91\",\"campaign\":\"1\",\"pdays\":\"-1\",\"previous\":\"0\",\"poutcome\":\"unknown\"},\n",
    "                            {\"age\":\"30\",\"job\":\"services\",\"marital\":\"married\",\"education\":\"secondary\",\"default\":\"no\",\"balance\":\"586\",\"housing\":\"yes\",\"loan\":\"no\",\"contact\":\"cellular\",\"day\":\"20\",\"month\":\"nov\",\"duration\":\"148\",\"campaign\":\"6\",\"pdays\":\"176\",\"previous\":\"4\",\"poutcome\":\"failure\"},\n",
    "]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d @instances.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai endpoints predict $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=instances.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43342a43176e"
   },
   "source": [
    "## Clean up Vertex and BigQuery resources\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Cloud Storage Bucket\n",
    "- Model from CustomJob pipeline\n",
    "- Model from HyperparameterTuningJob pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad8d12061a65"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model resources\n",
    "custom_job_model = aiplatform.Model(CUSTOM_JOB_MODEL)\n",
    "hpt_job_model = aiplatform.Model(HPT_JOB_MODEL)\n",
    "custom_job_model.delete()\n",
    "hpt_job_model.delete()\n",
    "\n",
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tabnet_on_vertex_pipelines.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
